import os
import logging
from typing import List, Tuple, Dict, Any
import json
import time
from config import LLAMA_SCOUT_MODEL, TEMPERATURE, MAX_TOKENS, ERROR_MESSAGES

logger = logging.getLogger(__name__)

# Import Cerebras SDK - this is required for production
try:
    from cerebras.cloud.sdk import Cerebras
    CEREBRAS_SDK_AVAILABLE = True
except ImportError as e:
    logger.error(f"Cerebras SDK not available: {str(e)}")
    logger.error("Install with: pip install cerebras-cloud-sdk")
    CEREBRAS_SDK_AVAILABLE = False


def process_with_llama_scout(
    query: str,
    regulation_data: List[str],
    api_key: str
) -> Tuple[str, List[int]]:
    """
    Process regulation data with Llama Scout model via Cerebras API.
    
    Args:
        query: The user's query about automotive regulations
        regulation_data: List of extracted regulation content
        api_key: Cerebras API key
    
    Returns:
        Tuple containing:
            - The answer text generated by the model
            - List of source indices used in the response
    """
    logger.info("Processing regulation data with Llama Scout model")
    
    if not regulation_data:
        logger.error("No regulation data provided for processing")
        raise ValueError(ERROR_MESSAGES["no_data_found"])
    
    if not CEREBRAS_SDK_AVAILABLE:
        logger.error("Cerebras SDK is not available")
        raise Exception("Cerebras SDK is not installed. Run: pip install cerebras-cloud-sdk")
    
    if not api_key or api_key == "YOUR_CEREBRAS_API_KEY":
        logger.error("Cerebras API key not configured")
        raise ValueError(ERROR_MESSAGES["api_key_missing"])
    
    try:
        # Combine all regulation data with source indicators
        context = ""
        for i, data in enumerate(regulation_data):
            context += f"[Source {i}]\n{data}\n\n"
        
        # Create the prompt for the model
        prompt = create_prompt(query, context)
        
        # Call the Cerebras API with Llama Scout model
        answer, source_indices = call_cerebras_api(prompt, api_key)
        
        logger.info(f"Successfully processed with Llama Scout model. Referenced {len(source_indices)} sources.")
        return answer, source_indices
    
    except Exception as e:
        logger.error(f"Error in process_with_llama_scout: {str(e)}")
        raise Exception(f"{ERROR_MESSAGES['cerebras_api_error']}: {str(e)}")


def create_prompt(query: str, context: str) -> str:
    """
    Create a prompt for the Llama Scout model.
    
    Args:
        query: The user's query about automotive regulations
        context: Combined regulation data with source indicators
    
    Returns:
        Formatted prompt string
    """
    return f"""You are an Automotive Regulatory Expert with 30 years of experience in global homologation and compliance. 
Your task is to answer the user's question about automotive regulations based ONLY on the provided sources.

Sources:
{context}

Important instructions:
1. ONLY use information from the provided sources to answer the question.
2. If the sources don't contain relevant information, say "I don't have enough information from reliable regulatory sources to answer this question."
3. Be specific about which source you are using by referencing the source number like [Source 0], [Source 1], etc.
4. Don't make up any information. Only rely on the provided sources.
5. For any regulatory information, indicate which regions/countries it applies to.
6. Format your answer in clear paragraphs with relevant headings where appropriate.
7. At the end of your answer, list the source numbers you referenced.
8. If you find conflicting information between sources, note the discrepancy and reference both sources.
9. Always mention if information may be outdated and recommend checking with official authorities.
10. For specific legal compliance, always recommend consulting with qualified regulatory experts.

User question: {query}

Expert answer:"""


def call_cerebras_api(prompt: str, api_key: str) -> Tuple[str, List[int]]:
    """
    Call the Cerebras API with the Llama Scout model.
    
    Args:
        prompt: The formatted prompt
        api_key: Cerebras API key
    
    Returns:
        Tuple containing:
            - The answer text generated by the model
            - List of source indices used in the response
    """
    if not CEREBRAS_SDK_AVAILABLE:
        raise Exception("Cerebras SDK is not available")
    
    try:
        client = Cerebras(api_key=api_key)
        
        # Call the Cerebras API with the Llama Scout model
        response = client.chat.completions.create(
            messages=[
                {"role": "user", "content": prompt}
            ],
            model=LLAMA_SCOUT_MODEL,
            temperature=TEMPERATURE,
            max_tokens=MAX_TOKENS
        )
        
        # Extract the generated text
        answer = response.choices[0].message.content
        
        # Extract source indices from the response
        source_indices = extract_source_indices(answer)
        
        # Validate that the response contains source references
        if not source_indices:
            logger.warning("LLM response did not reference any sources")
            answer += "\n\n*Note: This response was generated from the provided sources but specific source references may not be explicit.*"
        
        return answer, source_indices
        
    except Exception as e:
        logger.error(f"Error calling Cerebras API: {str(e)}")
        raise Exception(f"Cerebras API error: {str(e)}")


def extract_source_indices(text: str) -> List[int]:
    """
    Extract source indices referenced in the model's response.
    
    Args:
        text: The model's response text
    
    Returns:
        List of source indices
    """
    import re
    
    # Find all instances of [Source X] in the text
    matches = re.findall(r'\[Source\s+(\d+)\]', text)
    
    # Convert matches to integers and remove duplicates
    source_indices = list(set(int(match) for match in matches))
    
    return sorted(source_indices)


def validate_response_quality(response: str, query: str) -> bool:
    """
    Validate the quality of the LLM response.
    
    Args:
        response: The LLM response
        query: The original query
    
    Returns:
        bool: True if response quality is acceptable
    """
    # Check for minimum response length
    if len(response.strip()) < 50:
        return False
    
    # Check if response actually addresses the query
    query_words = set(query.lower().split())
    response_words = set(response.lower().split())
    
    # At least some overlap between query and response
    overlap = len(query_words.intersection(response_words))
    if overlap < 2:
        return False
    
    # Check for common non-informative responses
    non_informative_phrases = [
        "i don't know",
        "i cannot answer",
        "insufficient information",
        "no information available"
    ]
    
    response_lower = response.lower()
    if any(phrase in response_lower for phrase in non_informative_phrases):
        return False
    
    return True


def enhance_response_with_disclaimers(response: str) -> str:
    """
    Add appropriate disclaimers to the response.
    
    Args:
        response: The original response
    
    Returns:
        Enhanced response with disclaimers
    """
    disclaimer = """

**Important Disclaimers:**
- This information is based on available regulatory sources and may not reflect the most current regulations
- Always verify with official regulatory authorities before making compliance decisions
- For legal compliance matters, consult with qualified automotive regulatory experts
- Regulations are subject to change and may vary by jurisdiction"""
    
    return response + disclaimer
